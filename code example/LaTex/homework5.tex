\documentclass[12pt]{article}

\title{CS177 Homework 5: Rare Events \& Monte Carlo}
\author{Yating Liu 10588498}
\date{2018/11/28}

\setlength{\topmargin}{0.0in}
\setlength{\headheight}{0.0in}
\setlength{\headsep}{0.0in}
\setlength{\textheight}{9.0in}
\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.5in}

\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{physics}
     
\begin{document}

\maketitle

\noindent

\subsubsection*{Question 1:} 
\begin{enumerate}[a)]
  \item 
  $E[M_n]=t$, $var[M_n]=\dfrac{\sigma^2}{n}=\dfrac{3}{n}$, and by Chebyshev’s inequality
  \begin{align*}
  P(\abs{M_n-E[M_n]} \geq 0.5) &\leq \dfrac{var[M_n]}{0.5^2} = \dfrac{12}{n}\\
  1-P(\abs{M_n-E[M_n]} \geq 0.5) &\geq 1-\dfrac{12}{n}\\
  P(\abs{M_n-E[M_n]} \leq 0.5) &\geq 1-\dfrac{12}{n}
  \end{align*}
  In the question, we know 
  \begin{equation*}
  P(\abs{M_n-E[M_n]} \leq 0.5) \geq 0.99
  \end{equation*}
  Therefore,
  \begin{align*}
  1-\dfrac{12}{n} &\geq 0.99\\
  n &\geq 1200 
  \end{align*}
  
  \item 
  $\sigma = \sqrt{var[M_n]}=\dfrac{\sqrt{3}}{\sqrt{n}}$
  \begin{equation*}
  P\left(t-\dfrac{1}{2} \leq M_n \leq t+\dfrac{1}{2}\right)=P\left(-\dfrac{1}{2\sigma} \leq \dfrac{M_n-t}{\sigma} \leq \dfrac{1}{2\sigma}\right)=P\left(-\dfrac{\sqrt{3n}}{6} \leq Z_n \leq \dfrac{\sqrt{3n}}{6}\right)
  \end{equation*}
  Because $Z_n$ is also normal random variable, the distribution of $Z_n$ has $E[Z_n]=0$ and $var[Z_n]=1$ 
  \begin{equation*}
  P\left(-\dfrac{\sqrt{3n}}{6} \leq Z_n \leq \dfrac{\sqrt{3n}}{6}\right)=\Phi\left({\dfrac{\sqrt{3n}}{6}}\right)- \Phi\left({-\dfrac{\sqrt{3n}}{6}}\right)=1-2\Phi\left({-\dfrac{\sqrt{3n}}{6}}\right)=0.99
  \end{equation*}
  Therefore, $\Phi\left({-\dfrac{\sqrt{3n}}{6}}\right)=0.005$, $n=\left(\dfrac{scipy.stats.norm.ppf(0.005) \times -6}{\sqrt{3}}\right)^2 =80$
  \item
  Chebyshev’s inequality needs much more samples and is guaranteed that the bound is hold. Chebyshev’s inequality also holds the bound for any random variable X, while CLT holds the bound only for fixed $M_n$.
  

 
\end{enumerate}

\subsubsection*{Question 2:} 
\begin{enumerate}[a)]
  \item 
  By the axioms of probability, the upper bound is 1. $P(X \geq a) \leq 1$
  
  \item 
  By Markov's inequality, for any a, $P(X \geq a) \leq \dfrac{E[X]}{a}=\dfrac{4}{a}$
  
  \item 
  Referred lec09\_centralLimit, "Markov’s Inequality: Another Proof"
  \begin{equation*}
  E[X^2]=\int_{0}^{\infty}x^2f(x)dx \geq \int_{a}^{\infty}x^2f(x)dx \geq \int_{a}^{\infty}a^2f(x)dx = a^2P(X \geq a)
  \end{equation*}
  Therefore, $P(X \geq a) \leq \dfrac{E[X^2]}{a^2}$, $g(a)=a^2$
  
  \item 
  Because $var[X]=E[X^2]-E[X]^2$, $E[X^2]=var[X]+E[X]^2=10+16=26$, the inequality in part(c) becomes $P(X \geq a) \leq \dfrac{26}{a^2}$. Therefore, when $\dfrac{26}{a^2} < \dfrac{4}{a}$, $a>\dfrac{13}{2}$, the bound in part(c) is tighter, when $\dfrac{26}{a^2} > \dfrac{4}{a}$ and $\dfrac{4}{a} \leq 1$, $4 \leq a<6.5$, the bound in part(b) is tighter. 



\end{enumerate}


\subsubsection*{Question 3:} 
\begin{enumerate}[a)]
  \item 
  For samples follow a uniform distribution on [0,1], $E[\bar{X}] = \mu = 0.5 $, $var[\bar{X}]=\dfrac{\sigma^2}{n} = \dfrac{1}{12000}$, $\sigma \approx 0.009$. The approximate probability of their average being greater than or equal to 0.55 
  \begin{equation*}
  P(\bar{X}\geq 0.55)=P(\dfrac{\bar{X}-\mu}{\sigma}\geq \dfrac{0.55-\mu}{\sigma})=P(\bar{Z}\geq 5.48)
  \end{equation*}
  $P(\bar{Z}\geq 5.48) = 1-\Phi(5.48)=2\times 10^{-8}$. Because the probability is too small, the random number generator is not correct. 
  \item 
  The sample mean exactly equals to the true mean does not mean the random number generator is correct. The code could only produce 0.5, in which case the sample mean equals to the true mean but the numbers do not follow uniform distribution. 


\end{enumerate}

\subsubsection*{Question 4:} 
\begin{enumerate}[a)]
  \item 
  In the worst case, the original list is in decreasing order, so each number has to compare with all its preceding elements. The maximum possible number of comparisons is $1+2+ \ldots +(n-1)=\dfrac{n(n-1)}{2}$
     
  \item 
  Suppose $X_i$ is the number of comparisons needed for element i to bubble to the right position j. Because the whole list is randomized, the probability of being in any of the position in the list is the same, thus the probability of $X_i$ is the same. If $j=1$, $X_i=i-1$, if $j \neq 1$, $1 \leq X_i \leq i-j+1$.
  \begin{equation*}
  E[X_i] = \dfrac{1}{i}(i-1)+\dfrac{1}{i}(i-2+1)+\dfrac{1}{i}(i-3+1)+\dfrac{1}{i}(i-4+1) \ldots +\dfrac{1}{i}\cdot 1=\dfrac{1}{i}(i-1+\dfrac{i(i-1)}{2}) = 1-\dfrac{1}{i}+\dfrac{i-1}{2}
  \end{equation*}
  \begin{equation*}
  E[X]=\sum_{i=1}^{n}E[X_i]=\sum_{i=1}^{n}\left(1-\dfrac{1}{i}+\dfrac{i-1}{2}\right)=\dfrac{n(n+3)}{4}-\sum_{i=1}^{n}\dfrac{1}{i}
  \end{equation*}

  \item 
  The average number of comparisons across 1000 Monte Carlo trials is 29.558. For $n = 10$, $E[X]$ in part(b) is 29.571.
  
  \item 
  The average number of comparisons across 1000 Monte Carlo trials is 2573.035. For $n = 100$, $E[X]$ in part(b) is 2569.813. As the list length grows, the sample mean becomes less close to the theoretical mean. 

  



\end{enumerate}
  
  

\end{document}